{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 1: Neural Networks\n",
    "\n",
    "Implement your code and answer all the questions. Once you complete the assignment and answer the questions inline, you can download the report in pdf (File->Download as->PDF) and send it to us, together with the code. \n",
    "\n",
    "**Don't submit additional cells in the notebook, we will not check them. Don't change parameters of the learning inside the cells.**\n",
    "\n",
    "Assignment 1 consists of 4 sections:\n",
    "* **Section 1**: Data Preparation\n",
    "* **Section 2**: Multinomial Logistic Regression\n",
    "* **Section 3**: Backpropagation\n",
    "* **Section 4**: Neural Networks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Import necessary standard python packages \n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Setting configuration for matplotlib\n",
    "%matplotlib inline\n",
    "plt.rcParams['figure.figsize'] = (10.0, 8.0)\n",
    "plt.rcParams['image.interpolation'] = 'nearest'\n",
    "plt.rcParams['image.cmap'] = 'gray'\n",
    "plt.rcParams['xtick.labelsize'] = 15\n",
    "plt.rcParams['ytick.labelsize'] = 15\n",
    "plt.rcParams['axes.labelsize'] = 20\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Import python modules for this assignment\n",
    "\n",
    "from uva_code.cifar10_utils import get_cifar10_raw_data, preprocess_cifar10_data\n",
    "from uva_code.solver import Solver\n",
    "from uva_code.losses import SoftMaxLoss, CrossEntropyLoss, HingeLoss\n",
    "from uva_code.layers import LinearLayer, ReLULayer, SigmoidLayer, TanhLayer, SoftMaxLayer, ELULayer\n",
    "from uva_code.models import Network\n",
    "from uva_code.optimizers import SGD\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1:  Data Preparation\n",
    "\n",
    "In this section you will download [CIFAR10](https://www.cs.toronto.edu/~kriz/cifar.html \"CIFAR10\") data which you will use in this assignment. \n",
    "\n",
    "**Make sure that everything has been downloaded correctly and all images are visible.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get raw CIFAR10 data. For Unix users the script to download CIFAR10 dataset (get_cifar10.sh).\n",
    "# Try to run script to download the data. It should download tar archive, untar it and then remove it. \n",
    "# If it is doesn't work for some reasons (like Permission denied) then manually download the data from \n",
    "# http://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz and extract it to cifar-10-batches-py folder inside \n",
    "# cifar10 folder.\n",
    "\n",
    "X_train_raw, Y_train_raw, X_test_raw, Y_test_raw = get_cifar10_raw_data()\n",
    "\n",
    "#Checking shapes, should be (50000, 32, 32, 3), (50000, ), (10000, 32, 32, 3), (10000, )\n",
    "print(\"Train data shape: {0}\").format(str(X_train_raw.shape))\n",
    "print(\"Train labels shape: {0}\").format(str(Y_train_raw.shape))\n",
    "print(\"Test data shape: {0}\").format(str(X_test_raw.shape))\n",
    "print(\"Test labels shape: {0}\").format(str(Y_test_raw.shape))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize CIFAR10 data\n",
    "samples_per_class = 10\n",
    "classes = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "num_classes = len(classes)\n",
    "can = np.zeros((320, 320, 3),dtype='uint8')\n",
    "for i, cls in enumerate(classes):\n",
    "    idxs = np.flatnonzero(Y_train_raw == i) \n",
    "    idxs = np.random.choice(idxs, samples_per_class, replace = False)\n",
    "    for j in range(samples_per_class):\n",
    "        can[32 * i:32 * (i + 1), 32 * j:32 * (j + 1),:] = X_train_raw[idxs[j]]\n",
    "plt.xticks([], [])\n",
    "plt.yticks(range(16, 320, 32), classes)\n",
    "plt.title('CIFAR10', fontsize = 20)\n",
    "plt.imshow(can)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Normalize CIFAR10 data by subtracting the mean image. With these data you will work in the rest of assignment.\n",
    "# The validation subset will be used for tuning the hyperparameters.\n",
    "X_train, Y_train, X_val, Y_val, X_test, Y_test = preprocess_cifar10_data(X_train_raw, Y_train_raw, \n",
    "                                                                         X_test_raw, Y_test_raw, num_val = 1000)\n",
    "\n",
    "#Checking shapes, should be (49000, 3072), (49000, ), (1000, 3072), (1000, ), (10000, 3072), (10000, ) \n",
    "print(\"Train data shape: {0}\".format(str(X_train.shape))\n",
    "print(\"Train labels shape: {0}\".format(str(Y_train.shape))\n",
    "print(\"Val data shape: {0}\".format(str(X_val.shape))\n",
    "print(\"Val labels shape: {0}\".format(str(Y_val.shape))\n",
    "print(\"Test data shape: {0}\".format(str(X_test.shape))\n",
    "print(\"Test labels shape: {0}\".format(str(Y_test.shape))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation: Question 1 [4 points]\n",
    "\n",
    "Neural networks and deep learning methods prefer the input variables to contain as raw data as possible. \n",
    "But in the vast majority of cases data need to be preprocessed. Suppose, you have two types of non-linear  activation functions ([Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) and two types of normalization ([Per-example mean substraction](http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing#Per-example_mean_subtraction), [Standardization](http://ufldl.stanford.edu/wiki/index.php/Data_Preprocessing#Feature_Standardization)). What type of preprocessing you would prefer to use for each activation function and why? For example, in the previous cell we used per-example mean substraction.\n",
    "\n",
    "**Your Answer**: Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2: Multinomial Logistic Regression [5 points]\n",
    "\n",
    "In this section you will get started by implementing a linear classification model called [Multinomial Logistic Regression](http://ufldl.stanford.edu/wiki/index.php/Softmax_Regression). Later on you will extend this model to a neural network. You will train it by using the [mini-batch Stochastic Gradient Descent algorithm](http://sebastianruder.com/optimizing-gradient-descent/index.html#minibatchgradientdescent). You should implement how to sample batches, how to compute the loss, how to compute the gradient of the loss with respect to the parameters of the model and how to update the parameters of the model. \n",
    "\n",
    "You should get around 0.35 accuracy on the validation and test sets with the provided parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DONT CHANGE THE SEED AND THE DEFAULT PARAMETERS. OTHERWISE WE WILL NOT BE ABLE TO CORRECT YOUR ASSIGNMENT!\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Default parameters. \n",
    "num_iterations = 1500\n",
    "val_iteration = 100\n",
    "batch_size = 200\n",
    "learning_rate = 1e-7\n",
    "weight_decay = 3e+4\n",
    "weight_scale = 0.0001\n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Initialize the weights W using a normal distribution with mean = 0 and std =         #\n",
    "# weight_scale. Initialize the biases b with 0.                                        #\n",
    "########################################################################################   \n",
    "W = None\n",
    "b = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "\n",
    "train_loss_history = []\n",
    "train_acc_history = []\n",
    "\n",
    "val_loss_history = []\n",
    "val_acc_history = []\n",
    "\n",
    "for iteration in range(num_iterations):\n",
    "    ########################################################################################\n",
    "    # TODO:                                                                                #\n",
    "    # Sample a random mini-batch with the size of batch_size from the train set. Put the   #\n",
    "    # images to X_train_batch and labels to Y_train_batch variables.                       #\n",
    "    ########################################################################################\n",
    "    X_train_batch = None\n",
    "    Y_train_batch = None\n",
    "    ########################################################################################\n",
    "    #                              END OF YOUR CODE                                        #\n",
    "    ########################################################################################\n",
    "    \n",
    "    ########################################################################################\n",
    "    # TODO:                                                                                #\n",
    "    # Compute the loss and the accuracy of the multinomial logistic regression classifier  #\n",
    "    # on X_train_batch, Y_train_batch. The loss should be an average of the losses on all  #\n",
    "    # samples in the mini-batch. Include to the loss L2-regularization over the weight     #\n",
    "    # matrix W with regularization parameter equals to weight_decay.                       #            \n",
    "    ########################################################################################\n",
    "    train_loss = None\n",
    "    train_acc = None\n",
    "    ########################################################################################\n",
    "    #                              END OF YOUR CODE                                        #\n",
    "    ########################################################################################\n",
    "    \n",
    "    ########################################################################################\n",
    "    # TODO:                                                                                #\n",
    "    # Compute the gradients of the loss with the respect to the weights and biases. Put    #\n",
    "    # them in dW and db variables.                                                         #\n",
    "    ########################################################################################\n",
    "    dW = None\n",
    "    db = None\n",
    "    ########################################################################################\n",
    "    #                              END OF YOUR CODE                                        #\n",
    "    ########################################################################################\n",
    "    \n",
    "    ########################################################################################\n",
    "    # TODO:                                                                                #\n",
    "    # Update the weights W and biases b using the Stochastic Gradient Descent update rule. #\n",
    "    ########################################################################################\n",
    "\n",
    "    ########################################################################################\n",
    "    #                              END OF YOUR CODE                                        #\n",
    "    ########################################################################################\n",
    "    \n",
    "    if iteration % val_iteration == 0 or iteration == num_iterations - 1:\n",
    "        ########################################################################################\n",
    "        # TODO:                                                                                #\n",
    "        # Compute the loss and the accuracy on the validation set.                             #\n",
    "        ########################################################################################\n",
    "        val_loss = None\n",
    "        val_acc = None\n",
    "        ########################################################################################\n",
    "        #                              END OF YOUR CODE                                        #\n",
    "        ########################################################################################\n",
    "        train_loss_history.append(train_loss)\n",
    "        train_acc_history.append(train_acc)\n",
    "        val_loss_history.append(val_loss)\n",
    "        val_acc_history.append(val_acc)\n",
    "        \n",
    "        # Output loss and accuracy during training\n",
    "        print(\"Iteration {0:d}/{1:d}. Train Loss = {2:.3f}, Train Accuracy = {3:.3f}\".\n",
    "              format(iteration, num_iterations, train_loss, train_acc))\n",
    "        print(\"Iteration {0:d}/{1:d}. Validation Loss = {2:.3f}, Validation Accuracy = {3:.3f}\".\n",
    "              format(iteration, num_iterations, val_loss, val_acc))\n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Compute the accuracy on the test set.                                                #\n",
    "########################################################################################\n",
    "test_acc = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "print(\"Test Accuracy = {0:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Visualize a learning curve of multinomial logistic regression classifier\n",
    "plt.subplot(2, 1, 1)\n",
    "plt.plot(range(0, num_iterations + 1, val_iteration), train_loss_history, '-o', label = 'train')\n",
    "plt.plot(range(0, num_iterations + 1, val_iteration), val_loss_history, '-o', label = 'validation')\n",
    "plt.xlabel('Iteration')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc='upper right')\n",
    "\n",
    "plt.subplot(2, 1, 2)\n",
    "plt.plot(range(0, num_iterations + 1, val_iteration), train_acc_history, '-o', label='train')\n",
    "plt.plot(range(0, num_iterations + 1, val_iteration), val_acc_history, '-o', label='validation')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.gcf().set_size_inches(15, 12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression: Question 1 [4 points]\n",
    "\n",
    "What is the value of the loss and the accuracy you expect to obtain at iteration = 0 and why? Consider weight_decay = 0.\n",
    "\n",
    "**Your Answer**: Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression: Question 2 [4 points]\n",
    "\n",
    "Name at least three factors that determine the size of batches in practice and briefly motivate your answers. The factors might be related to computational or performance aspects.\n",
    "\n",
    "**Your Answer**: Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mulinomial Logistic Regression: Question 3 [4 points]\n",
    "\n",
    "Does the learning rate depend on the batch size? Explain how you should change the learning rate with respect to changes of the batch size.\n",
    "\n",
    "Name two extreme choices of a batch size and explain their advantages and disadvantages.\n",
    "\n",
    "**Your Answer**: Put your answer here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Multinomial Logistic Regression: Question 4 [4 points]\n",
    "\n",
    "Suppose that the weight matrix W has the shape (num_features, num_classes). How can you describe the columns of the weight matrix W? What are they representing? Why? \n",
    "\n",
    "**Your Answer**: Put your answer here.\n",
    "\n",
    "**Hint**: Before answering the question visualize the columns of the weight matrix W in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Visualize the learned weights for each class.                                        #\n",
    "########################################################################################\n",
    "\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3: Backpropagation\n",
    "\n",
    "Follow the instructions and solve the tasks in paper_assignment_1.pdf. Write your solutions in a separate pdf file. You don't need to put anything here.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4: Neural Networks [10 points]\n",
    "\n",
    "A modular implementation of neural networks allows to define deeper and more flexible architectures. In this section you will implement the multinomial logistic regression classifier from the Section 2 as a one-layer neural network that consists of two parts: a linear transformation layer (module 1) and a softmax loss layer (module 2).\n",
    "\n",
    "You will implement the multinomial logistic regression classifier as a modular network by following next steps:\n",
    "\n",
    "1. Implement the forward and backward passes for the linear layer in **layers.py** file. Write your code inside the ***forward*** and ***backward*** methods of ***LinearLayer*** class. Compute the regularization loss of the weights inside the ***layer_loss*** method of ***LinearLayer*** class. \n",
    "2. Implement the softmax loss computation in **losses.py** file. Write your code inside the ***SoftMaxLoss*** function. \n",
    "3. Implement the ***forward***, ***backward*** and ***loss*** methods for the ***Network*** class inside the **models.py** file.\n",
    "4. Implement the SGD update rule inside ***SGD*** class in **optimizers.py** file.\n",
    "5. Implement the ***train_on_batch***, ***test_on_batch***, ***fit***, ***predcit***, ***score***, ***accuracy*** methods of ***Solver*** class in ***solver.py*** file.\n",
    "\n",
    "**All computations should be implemented in vectorized. Don't loop over samples in the mini-batch.**\n",
    "\n",
    "You should get the same results for the next cell as in Section 2. **Don't change the parameters**.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DONT CHANGE THE SEED AND THE DEFAULT PARAMETERS. OTHERWISE WE WILL NOT BE ABLE TO CORRECT YOUR ASSIGNMENT!\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Default parameters. \n",
    "num_iterations = 1500\n",
    "val_iteration = 100\n",
    "batch_size = 200\n",
    "learning_rate = 1e-7\n",
    "weight_decay = 3e+4\n",
    "weight_scale = 0.0001\n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Build the multinomial logistic regression classifier using the Network model. You    #\n",
    "# will need to use add_layer and add_loss methods. Train this model using Solver class #\n",
    "# with SGD optimizer. In configuration of the optimizer you need to specify only       #\n",
    "# learning rate. Use the fit method to train classifier. Don't forget to include       #\n",
    "# X_val and Y_val in arguments to output the validation loss and accuracy during       #\n",
    "# training. Set the verbose to True to compare with the  multinomial logistic          #\n",
    "# regression classifier from the Section 2.                                            #\n",
    "########################################################################################\n",
    "model = None\n",
    "optimizer = None\n",
    "optimizer_config = None\n",
    "solver = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Compute the accuracy on the test set.                                                #\n",
    "########################################################################################\n",
    "test_acc = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "print(\"Test Accuracy = {0:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks: Task 1 [5 points]\n",
    "\n",
    "Tuning hyperparameters is very important even for multinomial logistic regression. \n",
    "\n",
    "What are the best learning rate and weight decay which produces the highest accuracy on the validation set? What is test accuracy of the model trained with the found best hyperparameters values?\n",
    "\n",
    "**Your Answer**: Put your answer here.\n",
    "\n",
    "***Hint:*** You should be able to get the test accuracy more than 0.4.\n",
    "\n",
    "Implement the tuning of hyperparameters (learning rate and weight decay) in the next cell. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# DONT CHANGE THE SEED AND THE DEFAULT PARAMETERS. OTHERWISE WE WILL NOT BE ABLE TO CORRECT YOUR ASSIGNMENT!\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Default parameters. \n",
    "num_iterations = 1500\n",
    "val_iteration = 100\n",
    "batch_size = 200\n",
    "weight_scale = 0.0001\n",
    "\n",
    "# You should try diffierent range of hyperparameters. \n",
    "learning_rates = [1e-7, 1e-8]\n",
    "weight_decays = [0, 3e+04]\n",
    "\n",
    "best_val_acc = -1\n",
    "best_solver = None\n",
    "for learning_rate in learning_rates:\n",
    "    for weight_decay in weight_decays:\n",
    "        ########################################################################################\n",
    "        # TODO:                                                                                #\n",
    "        # Implement the tuning of hyperparameters for the multinomial logistic regression. Save#\n",
    "        # maximum of the validation accuracy in best_val_acc and corresponding solver to       #\n",
    "        # best_solver variables. Store the maximum of the validation score for the current     #\n",
    "        # setting of the hyperparameters in cur_val_acc variable.                              #\n",
    "        ########################################################################################\n",
    "        cur_val_acc = None\n",
    "        ########################################################################################\n",
    "        #                              END OF YOUR CODE                                        #\n",
    "        ########################################################################################\n",
    "        print(\"Learning rate = {0:e}, weight decay = {1:e}: Validation Accuracy = {2:.3f}\".format(\n",
    "            learning_rate, weight_decay, cur_val_acc))    \n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Compute the accuracy on the test set for the best solver.                          #\n",
    "########################################################################################\n",
    "test_acc = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "print(\"Best Test Accuracy = {0:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks: Task 2 [5 points]\n",
    "\n",
    "Implement a two-layer neural network with a ReLU activation function. Write your code for the ***forward*** and ***backward*** methods of ***ReLULayer*** class in **layers.py** file.\n",
    "\n",
    "Train the network with the following structure: linear_layer-relu-linear_layer-softmax_loss. You should get the accuracy on the test set around 0.44. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONT CHANGE THE SEED AND THE DEFAULT PARAMETERS. OTHERWISE WE WILL NOT BE ABLE TO CORRECT YOUR ASSIGNMENT!\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of hidden units in a hidden layer.\n",
    "num_hidden_units = 100\n",
    "\n",
    "# Default parameters. \n",
    "num_iterations = 1500\n",
    "val_iteration = 100\n",
    "batch_size = 200\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 0\n",
    "weight_scale = 0.0001\n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Build the model with the structure: linear_layer-relu-linear_layer-softmax_loss.     #\n",
    "# Train this model using Solver class with SGD optimizer. In configuration of the      #\n",
    "# optimizer you need to specify only the learning rate. Use the fit method to train.   # \n",
    "########################################################################################\n",
    "model = None\n",
    "optimizer = None\n",
    "optimizer_config = None\n",
    "solver = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "    \n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Compute the accuracy on the test set.                                                #\n",
    "########################################################################################\n",
    "test_acc = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "print(\"Test Accuracy = {0:.3f}\".format(test_acc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks: Task 3 [5 points]\n",
    "\n",
    "Why the ReLU layer is important? What will happen if we exclude this layer? What will be the accuracy on the test set?\n",
    "\n",
    "**Your Answer**: Put your answer here.\n",
    "    \n",
    "Implement other activation functions: [Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function), [Tanh](https://en.wikipedia.org/wiki/Hyperbolic_function#Hyperbolic_tangent) and [ELU](https://arxiv.org/pdf/1511.07289v3.pdf) functions. \n",
    "Write your code for the ***forward*** and ***backward*** methods of ***SigmoidLayer***, ***TanhLayer*** and ***ELULayer*** classes in **layers.py** file.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONT CHANGE THE SEED AND THE DEFAULT PARAMETERS. OTHERWISE WE WILL NOT BE ABLE TO CORRECT YOUR ASSIGNMENT!\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Number of hidden units in a hidden layer. \n",
    "num_hidden_units = 100\n",
    "\n",
    "# Default parameters. \n",
    "num_iterations = 1500\n",
    "val_iteration = 100\n",
    "batch_size = 200\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 0\n",
    "weight_scale = 0.0001\n",
    "\n",
    "# Store results here\n",
    "results = {}\n",
    "layers_name = ['ReLU', 'Sigmoid', 'Tanh', 'ELU']\n",
    "layers = [ReLULayer(), SigmoidLayer(), TanhLayer(), ELULayer()]\n",
    "\n",
    "for layer_name, layer in zip(layers_name, layers):\n",
    "    ########################################################################################\n",
    "    # Build the model with the structure: linear_layer-activation-linear_layer-softmax_loss# \n",
    "    # Train this model using Solver class with SGD optimizer. In configuration of the      #\n",
    "    # optimizer you need  to specify only the learning rate. Use the fit method to train.  #\n",
    "    # Store validation history in results dictionary variable.                             # \n",
    "    ########################################################################################\n",
    "\n",
    "    ########################################################################################\n",
    "    #                              END OF YOUR CODE                                        #\n",
    "    ########################################################################################\n",
    "    results[layer_name] = val_acc_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Visualize a learning curve for different activation functions\n",
    "for layer_name in layers_name:\n",
    "    plt.plot(range(0, num_iterations + 1, val_iteration), results[layer_name], '-o', label = layer_name)\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks: Task 4 [10 points]\n",
    "\n",
    "Although typically a [Softmax](https://en.wikipedia.org/wiki/Softmax_function) layer is coupled with a [Cross Entropy loss](https://en.wikipedia.org/wiki/Cross_entropy#Cross-entropy_error_function_and_logistic_regression), this is not necessary and you can use a different loss function. Next, implement the network with the Softmax layer paired with a [Hinge loss](https://en.wikipedia.org/wiki/Hinge_loss). Beware, with the Softmax layer all the output dimensions depend on all the input dimensions, hence, you need to compute the Jacobian of derivatives $\\frac{\\partial o_i}{dx_j}$. \n",
    "\n",
    "Implement the ***forward*** and ***backward*** methods for \n",
    "***SoftMaxLayer*** in **layers.py** file and ***CrossEntropyLoss*** and ***HingeLoss*** in **losses.py** file. You should implement multi-class cross-entropy and hinge losses. \n",
    "\n",
    "Results of using SoftMaxLoss and SoftMaxLayer + CrossEntropyLoss should be the same.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# DONT CHANGE THE SEED AND THE DEFAULT PARAMETERS. OTHERWISE WE WILL NOT BE ABLE TO CORRECT YOUR ASSIGNMENT!\n",
    "# Seed\n",
    "np.random.seed(42)\n",
    "\n",
    "# Default parameters. \n",
    "num_iterations = 1500\n",
    "val_iteration = 100\n",
    "batch_size = 200\n",
    "learning_rate = 2e-3\n",
    "weight_decay = 0\n",
    "weight_scale = 0.0001\n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Build the model with the structure:                                                  #\n",
    "# linear_layer-relu-linear_layer-softmax_layer-hinge_loss.                             #\n",
    "# Train this model using Solver class with SGD optimizer. In configuration of the      #\n",
    "# optimizer you need to specify only the learning rate. Use the fit method to train.   # \n",
    "########################################################################################\n",
    "model = None\n",
    "optimizer = None\n",
    "optimizer_config = None\n",
    "solver = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "\n",
    "########################################################################################\n",
    "# TODO:                                                                                #\n",
    "# Compute the accuracy on the test set.                                                #\n",
    "########################################################################################\n",
    "test_acc = None\n",
    "########################################################################################\n",
    "#                              END OF YOUR CODE                                        #\n",
    "########################################################################################\n",
    "print(\"Test Accuracy = {0:.3f}\".format(test_acc))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
